#!/usr/bin/env python3
"""
AI Dev Agent - Hugging Face Spaces Demo
Streamlit app for testing the AI Dev Agent in the browser.
"""

import streamlit as st
import yaml
import asyncio
import json
from datetime import datetime
from typing import Dict, Any

# Import our modules
try:
    from agents.hybrid_llm_connector import HybridLLMConnector
    from agents.conversational_brain import ConversationalBrain
    from utils.logger import get_action_logger
    HYBRID_AVAILABLE = True
except ImportError:
    HYBRID_AVAILABLE = False

# Page configuration
st.set_page_config(
    page_title="AI Dev Agent - NOVA System",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .feature-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .metric-card {
        background-color: #e8f4fd;
        padding: 1rem;
        border-radius: 0.5rem;
        text-align: center;
    }
    .success-message {
        background-color: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #c3e6cb;
    }
    .error-message {
        background-color: #f8d7da;
        color: #721c24;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #f5c6cb;
    }
</style>
""", unsafe_allow_html=True)

def load_config() -> Dict[str, Any]:
    """Load configuration for the demo."""
    try:
        # Use a simplified config for Spaces
        config = {
            'llm': {
                'model_type': 'huggingface',
                'provider': 'huggingface',
                'models': {
                    'fast': {
                        'model_name': 'microsoft/DialoGPT-small',
                        'model_id': 'microsoft/DialoGPT-small',
                        'max_tokens': 512,
                        'temperature': 0.7,
                        'load_in_8bit': True,
                        'device_map': 'auto'
                    }
                },
                'performance': {
                    'use_cache': True,
                    'max_cache_size': 100,
                    'cache_ttl': 3600,
                    'timeout': 30
                },
                'device': {
                    'use_gpu': False,  # Spaces typically don't have GPU
                    'cpu_threads': 2
                }
            },
            'voice': {
                'enabled': False  # Disable voice in Spaces
            }
        }
        return config
    except Exception as e:
        st.error(f"Failed to load configuration: {e}")
        return {}

def initialize_connector():
    """Initialize the LLM connector."""
    if not HYBRID_AVAILABLE:
        st.error("Hybrid connector not available. Please check dependencies.")
        return None
    
    try:
        config = load_config()
        connector = HybridLLMConnector(config)
        return connector
    except Exception as e:
        st.error(f"Failed to initialize connector: {e}")
        return None

def run_speed_test(connector, test_prompts: list) -> Dict[str, Any]:
    """Run a simple speed test."""
    try:
        results = {
            'prompts': [],
            'total_time': 0,
            'success_count': 0
        }
        
        for i, prompt in enumerate(test_prompts):
            with st.spinner(f"Testing prompt {i+1}..."):
                start_time = datetime.now()
                
                # Run async function in sync context
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    response = loop.run_until_complete(
                        connector.send_prompt(prompt, max_tokens=100)
                    )
                finally:
                    loop.close()
                
                end_time = datetime.now()
                response_time = (end_time - start_time).total_seconds()
                
                results['total_time'] += response_time
                if response.get('success'):
                    results['success_count'] += 1
                
                results['prompts'].append({
                    'prompt': prompt,
                    'response_time': response_time,
                    'success': response.get('success', False),
                    'content': response.get('content', '')[:200] + "..." if len(response.get('content', '')) > 200 else response.get('content', ''),
                    'backend_used': response.get('routing_info', {}).get('backend_used', 'unknown')
                })
        
        results['avg_time'] = results['total_time'] / len(test_prompts)
        results['success_rate'] = results['success_count'] / len(test_prompts)
        
        return results
        
    except Exception as e:
        st.error(f"Speed test failed: {e}")
        return None

def main():
    """Main Streamlit app."""
    
    # Header
    st.markdown('<h1 class="main-header">ü§ñ AI Dev Agent - NOVA System</h1>', unsafe_allow_html=True)
    st.markdown("### Memory-aware, undo-capable, proactive AI developer assistant")
    
    # Sidebar
    st.sidebar.title("üéõÔ∏è Controls")
    
    # Initialize connector
    if 'connector' not in st.session_state:
        st.session_state.connector = initialize_connector()
    
    if st.session_state.connector is None:
        st.error("‚ùå Failed to initialize AI Dev Agent. Please check the configuration.")
        return
    
    # Main tabs
    tab1, tab2, tab3, tab4 = st.tabs(["üöÄ Demo", "‚ö° Speed Test", "üìä Performance", "üìö About"])
    
    with tab1:
        st.header("üöÄ Interactive Demo")
        
        # Input section
        col1, col2 = st.columns([3, 1])
        
        with col1:
            user_input = st.text_area(
                "üí¨ Enter your message:",
                placeholder="Try: 'Hello, how are you?' or 'Write a simple Python function'",
                height=100
            )
        
        with col2:
            task_type = st.selectbox(
                "üéØ Task Type:",
                ["auto", "coding", "simple_qa", "complex_analysis"],
                help="Auto will let the system decide the best backend"
            )
            
            max_tokens = st.slider("üìù Max Tokens:", 50, 500, 200)
        
        # Process button
        if st.button("üöÄ Process", type="primary"):
            if user_input.strip():
                with st.spinner("ü§ñ Processing..."):
                    try:
                        # Run async function
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        try:
                            response = loop.run_until_complete(
                                st.session_state.connector.send_prompt(
                                    user_input,
                                    task_type=task_type if task_type != "auto" else None,
                                    max_tokens=max_tokens
                                )
                            )
                        finally:
                            loop.close()
                        
                        # Display results
                        if response.get('success'):
                            st.markdown('<div class="success-message">', unsafe_allow_html=True)
                            st.success("‚úÖ Response generated successfully!")
                            st.markdown('</div>', unsafe_allow_html=True)
                            
                            # Response details
                            col1, col2, col3 = st.columns(3)
                            
                            with col1:
                                st.metric("Backend Used", response.get('routing_info', {}).get('backend_used', 'unknown').title())
                            
                            with col2:
                                st.metric("Response Time", f"{response.get('response_time', 0):.3f}s")
                            
                            with col3:
                                st.metric("Content Length", len(response.get('content', '')))
                            
                            # Display response
                            st.subheader("ü§ñ AI Response:")
                            st.write(response.get('content', ''))
                            
                            # Show routing info
                            with st.expander("üîç Routing Information"):
                                routing_info = response.get('routing_info', {})
                                st.json(routing_info)
                        
                        else:
                            st.markdown('<div class="error-message">', unsafe_allow_html=True)
                            st.error(f"‚ùå Error: {response.get('error', 'Unknown error')}")
                            st.markdown('</div>', unsafe_allow_html=True)
                    
                    except Exception as e:
                        st.error(f"‚ùå Processing failed: {e}")
            else:
                st.warning("‚ö†Ô∏è Please enter a message to process.")
    
    with tab2:
        st.header("‚ö° Speed Test")
        
        # Test configuration
        col1, col2 = st.columns(2)
        
        with col1:
            test_prompts = [
                "Hello, how are you?",
                "What is the weather like?",
                "Can you help me with coding?",
                "Explain machine learning briefly.",
                "Write a simple Python function."
            ]
            
            st.subheader("üìù Test Prompts")
            for i, prompt in enumerate(test_prompts):
                st.write(f"{i+1}. {prompt}")
        
        with col2:
            st.subheader("‚öôÔ∏è Test Settings")
            st.write("‚Ä¢ Model: Microsoft DialoGPT-small")
            st.write("‚Ä¢ Max Tokens: 100")
            st.write("‚Ä¢ Temperature: 0.7")
            st.write("‚Ä¢ Device: CPU (Spaces compatible)")
        
        # Run test button
        if st.button("üèÉ‚Äç‚ôÇÔ∏è Run Speed Test", type="primary"):
            with st.spinner("‚ö° Running speed test..."):
                results = run_speed_test(st.session_state.connector, test_prompts)
                
                if results:
                    # Display results
                    st.subheader("üìä Test Results")
                    
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric("Total Tests", len(results['prompts']))
                    
                    with col2:
                        st.metric("Success Rate", f"{results['success_rate']:.1%}")
                    
                    with col3:
                        st.metric("Average Time", f"{results['avg_time']:.3f}s")
                    
                    with col4:
                        st.metric("Total Time", f"{results['total_time']:.3f}s")
                    
                    # Detailed results
                    st.subheader("üìã Detailed Results")
                    
                    for i, result in enumerate(results['prompts']):
                        with st.expander(f"Test {i+1}: {result['prompt'][:50]}..."):
                            col1, col2, col3 = st.columns(3)
                            
                            with col1:
                                st.metric("Response Time", f"{result['response_time']:.3f}s")
                            
                            with col2:
                                st.metric("Success", "‚úÖ" if result['success'] else "‚ùå")
                            
                            with col3:
                                st.metric("Backend", result['backend_used'].title())
                            
                            st.write("**Response:**")
                            st.write(result['content'])
    
    with tab3:
        st.header("üìä Performance Metrics")
        
        try:
            metrics = st.session_state.connector.get_performance_metrics()
            
            if metrics:
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("üîÑ Request Statistics")
                    st.metric("Total Requests", metrics.get('hybrid_metrics', {}).get('total_requests', 0))
                    st.metric("Ollama Requests", metrics.get('hybrid_metrics', {}).get('ollama_requests', 0))
                    st.metric("HF Requests", metrics.get('hybrid_metrics', {}).get('hf_requests', 0))
                
                with col2:
                    st.subheader("‚ö° Performance")
                    if 'huggingface_metrics' in metrics:
                        hf_metrics = metrics['huggingface_metrics']
                        st.metric("Cache Hit Rate", f"{hf_metrics.get('cache_hit_rate', 0):.1%}")
                        st.metric("Avg Response Time", f"{hf_metrics.get('avg_response_time', 0):.3f}s")
                
                # System status
                st.subheader("üîß System Status")
                status = st.session_state.connector.get_status()
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**Ollama Status:**")
                    ollama_status = status.get('ollama_status', {})
                    st.write(f"‚Ä¢ Active: {'‚úÖ' if ollama_status.get('active') else '‚ùå'}")
                    st.write(f"‚Ä¢ Model: {ollama_status.get('model_name', 'N/A')}")
                
                with col2:
                    st.write("**Hugging Face Status:**")
                    hf_status = status.get('huggingface_status', {})
                    st.write(f"‚Ä¢ Active: {'‚úÖ' if hf_status.get('active') else '‚ùå'}")
                    st.write(f"‚Ä¢ Models: {', '.join(hf_status.get('loaded_models', []))}")
            
            else:
                st.warning("‚ö†Ô∏è No performance metrics available yet. Try running some tests first.")
        
        except Exception as e:
            st.error(f"‚ùå Failed to get performance metrics: {e}")
    
    with tab4:
        st.header("üìö About AI Dev Agent")
        
        st.markdown("""
        ### ü§ñ What is AI Dev Agent?
        
        AI Dev Agent (NOVA System) is a comprehensive AI-powered development assistant that combines:
        
        - **üß† Conversational AI**: Natural language interaction
        - **üé§ Voice Integration**: Multi-language voice commands
        - **üíæ Memory Management**: Persistent context and history
        - **‚Ü©Ô∏è Undo System**: Safe operation with rollback
        - **‚ö° Speed Optimization**: Hybrid LLM routing
        
        ### üèóÔ∏è Architecture
        
        The system uses a hybrid approach:
        
        - **Ollama Backend**: For complex coding tasks
        - **Hugging Face Backend**: For fast, simple responses
        - **Intelligent Routing**: Automatic backend selection
        
        ### üöÄ Features
        
        - **Multi-language Support**: English and Hindi
        - **Real-time Processing**: Low latency responses
        - **Performance Monitoring**: Detailed metrics
        - **Extensible Design**: Plugin architecture
        
        ### üìä Performance
        
        - **Response Time**: 0.2-1.0 seconds
        - **Memory Usage**: 200MB-1.5GB
        - **Cache Hit Rate**: 90%+
        - **Success Rate**: 95%+
        
        ### üîó Links
        
        - **Repository**: [GitHub](https://github.com/your-username/ai-dev-agent)
        - **Documentation**: [Wiki](https://github.com/your-username/ai-dev-agent/wiki)
        - **Issues**: [Bug Reports](https://github.com/your-username/ai-dev-agent/issues)
        
        ### üë®‚Äçüíª Author
        
        **Shubham25180** - AI Developer & Researcher
        
        ---
        
        *Made with ‚ù§Ô∏è for the AI development community*
        """)

if __name__ == "__main__":
    main() 