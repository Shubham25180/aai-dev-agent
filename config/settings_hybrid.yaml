# AI Dev Agent - Hybrid Configuration
# Best of both worlds: Ollama for coding, Hugging Face for speed testing

# Paths
paths:
  logs: logs
  memory: memory
  undo: undo
  voice: voice
  models: models/huggingface

# Logging configuration
logging:
  level: INFO
  format: "{time:YYYY-MM-DD HH:mm:ss} | {level} | {name} | {message}"
  file_rotation: "1 day"
  file_retention: "30 days"

# Hybrid LLM Configuration
llm:
  # Model selection strategy
  model_type: "hybrid" # hybrid, ollama, huggingface
  provider: "hybrid" # hybrid, ollama, huggingface
  model_name: "llama3.2:3b" # Set to match your available Ollama model

  # Routing configuration
  routing:
    default_backend: "ollama" # Default for unknown tasks
    # Remove or comment out simple_keywords to force all prompts to Ollama
    # simple_keywords:
    #   - "hello"
    #   - "hi"
    #   - "how are you"
    #   - "what is"
    #   - "explain briefly"
    #   - "quick"
    #   - "simple"
    #   - "basic"
    #   - "help"
    #   - "assist"
    #   - "weather"
    #   - "time"
    #   - "date"
    #   - "status"

    # Keywords that force Ollama (complex tasks)
    force_ollama:
      - "complex"
      - "detailed"
      - "analysis"
      - "review"
      - "compare"
      - "evaluate"
      - "assess"
      - "comprehensive"
      - "thorough"
      - "deep"
      - "extensive"

  # Ollama configuration (for coding tasks)
  ollama:
    model_type: "ollama"
    model_name: "llama3.2:3b" # Set to match your available Ollama model
    api_url: "http://localhost:11434"
    max_tokens: 2048
    temperature: 0.3 # Lower for coding
    timeout: 80
    cache_enabled: true
    max_cache_size: 1000
    cache_ttl: 3600

  # Hugging Face configuration (for speed testing)
  huggingface:
    model_type: "huggingface"
    provider: "huggingface"

    # Model configurations
    models:
      fast:
        model_name: "microsoft/DialoGPT-small"
        model_id: "microsoft/DialoGPT-small"
        max_tokens: 512
        temperature: 0.7
        load_in_8bit: true
        device_map: "auto"

      balanced:
        model_name: "microsoft/DialoGPT-medium"
        model_id: "microsoft/DialoGPT-medium"
        max_tokens: 1024
        temperature: 0.7
        load_in_8bit: true
        device_map: "auto"

    # Performance settings
    performance:
      use_cache: true
      max_cache_size: 1000
      cache_ttl: 3600
      batch_size: 1
      max_concurrent_requests: 5
      timeout: 30

    # Quantization settings
    quantization:
      load_in_8bit: true
      load_in_4bit: false
      bnb_4bit_compute_dtype: "float16"
      bnb_4bit_use_double_quant: true
      bnb_4bit_quant_type: "nf4"

    # Device settings
    device:
      use_gpu: true
      gpu_memory_fraction: 0.8
      cpu_threads: 4
      use_mps: false

    # Model loading strategy
    loading:
      preload_models: ["fast"]
      lazy_loading: true
      model_cache_dir: "models/huggingface"
      download_models: true

# Real-Time Voice System Configuration
voice:
  enabled: true
  model_path: model
  sample_rate: 16000
  min_confidence: 0.6
  tts_rate: 180
  tts_volume: 1.0
  auto_start: false

  # Real-time pipeline settings
  chunk_duration: 0.5 # seconds per audio chunk
  vad_threshold: 0.01 # Voice Activity Detection threshold
  silence_duration: 1.0 # seconds of silence to consider end of speech

  # Wake word settings
  wake_word: "nexus"
  wake_word_enabled: true

  # Multi-threading settings
  max_audio_queue_size: 100
  max_transcription_queue_size: 50
  max_command_queue_size: 20
  max_llm_queue_size: 10
  max_tts_queue_size: 10

  # Performance optimization
  use_streaming_whisper: true
  enable_voice_activity_detection: true
  enable_response_caching: true
  cache_ttl: 3600 # 1 hour

  # Language support
  supported_languages: ["en", "hi"]
  auto_language_detection: true

  # Commands for pattern matching
  commands:
    - pattern: "open (.+)"
      command: "open_file"
    - pattern: "run (.+)"
      command: "run_command"
    - pattern: "create (.+)"
      command: "create_file"
    - pattern: "delete (.+)"
      command: "delete_file"
    - pattern: "edit (.+)"
      command: "edit_file"
    - pattern: "undo"
      command: "undo"
    - pattern: "save"
      command: "save"
    - pattern: "quit|exit"
      command: "exit"
    - pattern: "file kholo (.+)"
      command: "open_file"
    - pattern: "script chalane (.+)"
      command: "run_command"
    - pattern: "function banane (.+)"
      command: "create_file"
    - pattern: "file delete (.+)"
      command: "delete_file"
    - pattern: "file edit (.+)"
      command: "edit_file"
    - pattern: "undo karo"
      command: "undo"
    - pattern: "save karo"
      command: "save"
    - pattern: "band karo"
      command: "exit"

# Safety settings
safety:
  require_confirmation: true
  max_file_size: 10485760 # 10MB
  allowed_extensions:
    [".py", ".js", ".html", ".css", ".json", ".yaml", ".txt", ".md"]
  blocked_commands: ["rm -rf", "format c:", "del /s /q"]

# Speed testing configuration
speed_testing:
  enabled: true
  test_models: ["fast", "balanced"]
  test_prompts:
    - "Hello, how are you?"
    - "What is the weather like?"
    - "Can you help me with coding?"
    - "Explain machine learning briefly."
    - "Write a simple Python function."
  metrics:
    track_response_time: true
    track_memory_usage: true
    track_gpu_usage: true
    track_cache_hits: true
  reporting:
    save_results: true
    results_file: "hybrid_speed_test_results.json"
    generate_report: true
    report_format: "markdown"

# Real-time performance targets
performance_targets:
  voice:
    max_transcription_latency: 0.5 # seconds
    max_llm_latency: 2.0 # seconds
    max_tts_latency: 0.3 # seconds
    max_total_latency: 3.0 # seconds
    min_throughput: 10 # audio chunks per second
  threading:
    min_active_threads: 5
    max_queue_utilization: 0.8
    thread_health_check_interval: 30 # seconds

os_mode: auto # auto, windows, linux, mac
