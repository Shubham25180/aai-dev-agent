# AI Dev Agent - Hugging Face Configuration
# Optimized for speed testing with various model sizes and quantization

# Paths
paths:
  logs: logs
  memory: memory
  undo: undo
  voice: voice
  models: models/huggingface # Local model cache

# Logging configuration
logging:
  level: INFO
  format: "{time:YYYY-MM-DD HH:mm:ss} | {level} | {name} | {message}"
  file_rotation: "1 day"
  file_retention: "30 days"

# Hugging Face LLM Configuration
llm:
  # Model selection strategy
  model_type: "huggingface" # huggingface, ollama, gpt4all, llama_cpp
  provider: "huggingface" # huggingface, local, api

  # Model configurations for speed testing
  models:
    # Fast models for quick responses
    fast:
      model_name: "microsoft/DialoGPT-small" # 117M parameters
      model_id: "microsoft/DialoGPT-small"
      max_tokens: 512
      temperature: 0.7
      load_in_8bit: true
      device_map: "auto"

    # Balanced models for general use
    balanced:
      model_name: "microsoft/DialoGPT-medium" # 345M parameters
      model_id: "microsoft/DialoGPT-medium"
      max_tokens: 1024
      temperature: 0.7
      load_in_8bit: true
      device_map: "auto"

    # Quality models for complex tasks
    quality:
      model_name: "microsoft/DialoGPT-large" # 774M parameters
      model_id: "microsoft/DialoGPT-large"
      max_tokens: 2048
      temperature: 0.7
      load_in_8bit: true
      device_map: "auto"

    # Code-specific models
    code:
      model_name: "microsoft/DialoGPT-medium" # Can be replaced with code-specific model
      model_id: "microsoft/DialoGPT-medium"
      max_tokens: 1024
      temperature: 0.3
      load_in_8bit: true
      device_map: "auto"

  # Performance settings
  performance:
    use_cache: true
    max_cache_size: 1000
    cache_ttl: 3600 # 1 hour
    batch_size: 1
    max_concurrent_requests: 5
    timeout: 30

  # Quantization settings for speed
  quantization:
    load_in_8bit: true
    load_in_4bit: false
    bnb_4bit_compute_dtype: "float16"
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: "nf4"

  # Device settings
  device:
    use_gpu: true
    gpu_memory_fraction: 0.8
    cpu_threads: 4
    use_mps: false # For Apple Silicon

  # Model loading strategy
  loading:
    preload_models: ["fast"] # Preload these models at startup
    lazy_loading: true # Load other models on demand
    model_cache_dir: "models/huggingface"
    download_models: true # Download from HF Hub if not cached

# Voice system configuration (keep existing)
voice:
  enabled: true
  model_path: model
  sample_rate: 16000
  min_confidence: 0.6
  tts_rate: 180
  tts_volume: 1.0
  auto_start: false
  commands:
    - pattern: "open (.+)"
      command: "open_file"
    - pattern: "run (.+)"
      command: "run_command"
    - pattern: "create (.+)"
      command: "create_file"
    - pattern: "delete (.+)"
      command: "delete_file"
    - pattern: "edit (.+)"
      command: "edit_file"
    - pattern: "undo"
      command: "undo"
    - pattern: "save"
      command: "save"
    - pattern: "quit|exit"
      command: "exit"

# Safety settings
safety:
  require_confirmation: true
  max_file_size: 10485760 # 10MB
  allowed_extensions:
    [".py", ".js", ".html", ".css", ".json", ".yaml", ".txt", ".md"]
  blocked_commands: ["rm -rf", "format c:", "del /s /q"]

# Speed testing configuration
speed_testing:
  enabled: true
  test_models: ["fast", "balanced", "quality", "code"]
  test_prompts:
    - "Hello, how are you?"
    - "What is the weather like?"
    - "Can you help me with coding?"
    - "Explain machine learning briefly."
    - "Write a simple Python function."
  metrics:
    track_response_time: true
    track_memory_usage: true
    track_gpu_usage: true
    track_cache_hits: true
  reporting:
    save_results: true
    results_file: "speed_test_results.json"
    generate_report: true
    report_format: "markdown"

os_mode: auto # auto, windows, linux, mac
